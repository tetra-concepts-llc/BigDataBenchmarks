There were a few things that I had to learn about to make this work.  The first issue that I ran into was Serialization with the Hadoop data type Text.  The Text data type is not Serializable.  This is an issue because Spark must be able to serialize data being sent to the work nodes.  It is core to it's design.  The work around for this is to convert the data from Text to String.  I tried to extend Serialiable to the Text class, but had no luck making this work.  

The second issue was the map/reduce code was trying to sort over 48 mappers.  While the map/reduce codes was sorting by the key (just like Spark), it was not sorting the same way.  Map/reduce will sort each part locally by default.  It will not perform a global sort unless you pass the correct partitioner.  This concept is further explained here ->  https://www.inkling.com/read/hadoop-definitive-guide-tom-white-3rd/chapter-8/sorting.  The correct way to handle this issue is with a partitioner that respects global sort order.  This is not the way I handled for this exersise.  I changed the number of reducers from 48 to 1.  This is not the best way to handle it because you would lose all benefits of running parallel.  This was not an issue because I am running local on 4GB of RAM.

The third issue I had is the codec that was chosen by Spark and Map/Reduce.  The Map/Reduce code was using the default codec when writing out sequence files.  Spark was not using any codec, causing the output not to be compressed.  The choice of codec determines how the output will be compressed.  Once I figured out how to pass the default codec class to the Spark save method, the output then was stored using the same compression.
